{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "methodologie_deep.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMmR/PhG1q4myytJLUrE825",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choarauc/form/blob/main/methodologie_deep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Charger le jeu de données 'face_images.csv' dans le dataframe df.\n",
        "Faire un bref audit des données."
      ],
      "metadata": {
        "id": "6zDa6WBaYlCK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UYGmY3iYc13"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('face_images.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour rappel, les images sont toutes stockées dans un dossier \"images\". Et, pour chaque image, df renseigne son nom et les coordonnées normalisées de l'objet. xmin, ymin, xmax et ymax représentent les coordonnées normalisées de la \"bounding box\" de l'objet, c'est-à-dire la boite qui englobe l'objet.\n",
        "\n",
        "\n",
        "\n",
        "xminxmin  : position horizontale de l'extrémité gauche de la boîte.\n",
        "yminymin  : position verticale de l'extrémité gauche de la boîte.\n",
        "xmaxxmax  : position horizontale de l'extrémité droite de la boîte.\n",
        "ymaxymax  : position verticale de l'extrémité droite de la boîte.\n",
        "\n",
        "Définir une fonction load_image avec comme argument filepath et resize qui charge, redimensionne et qui retourne l'image. Utiliser uniquement des fonctions de tensorflow.\n",
        "\n",
        "Charger et afficher la première image de df.\n",
        "Afficher ses dimensions."
      ],
      "metadata": {
        "id": "T51kQB6lYpjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "folder_images = 'images/'\n",
        "\n",
        "def load_image(filepath, resize=None):\n",
        "    im = tf.io.read_file(folder_images + filepath)\n",
        "    im = tf.image.decode_png(im, channels=3)\n",
        "    if resize:\n",
        "        return tf.image.resize(im, resize)\n",
        "    else :\n",
        "        return im\n",
        "\n",
        "        \n",
        "im = load_image(df.filename[0])\n",
        "plt.imshow(im);\n"
      ],
      "metadata": {
        "id": "Th-J_Ke-YnKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour localiser une unique boîte encadrant le visage dans l'image, nous pouvons nous ramener à un problème de régression sur les 4 variables cibles suivantes.\n",
        "\n",
        "\n",
        "\n",
        "xmoyxmoy  : position horizontale (normalisée) du milieu de la boîte.\n",
        "ymoyymoy  : position verticale (normalisée) du milieu de la boîte.\n",
        "ww  : largeur (normalisée) de la boîte.\n",
        "hh  : hauteur (normalisée) de la boîte.\n",
        "\n",
        "Créer les colonnes suivantes à df : xmoy, ymoy, w et h.\n",
        "Afficher les premières lignes de df."
      ],
      "metadata": {
        "id": "QXi7-molYvYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['xmoy'] = (df.xmax + df.xmin)/2\n",
        "df['ymoy'] = (df.ymax + df.ymin)/2\n",
        "df['w'] = (df.xmax - df.xmin)\n",
        "df['h'] = (df.ymax - df.ymin)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "kBM-atDaYtSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exécuter la cellule suivante pour afficher la bounding box de la première image."
      ],
      "metadata": {
        "id": "xvbvVuWkYzmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def show_bounding_box(im, bbox, normalised=True, color='r'):\n",
        "    # Signification de bbox\n",
        "    x, y, w, h = bbox\n",
        "    # Convertir les cordonées (x,y,w,h) en (x1,x2,y1,y2)\n",
        "    x1=x-w/2\n",
        "    x2=x+w/2\n",
        "    y1=y-h/2\n",
        "    y2=y+h/2\n",
        "    \n",
        "    # Redimentionner en cas de normalisation\n",
        "    if normalised:\n",
        "        x1=x1*im.shape[1]\n",
        "        x2=x2*im.shape[1]\n",
        "        y1=y1*im.shape[0]\n",
        "        y2=y2*im.shape[0]\n",
        "        \n",
        "    # Afficher l'image\n",
        "    plt.imshow(im)\n",
        "    \n",
        "    # Afficher la bounding box\n",
        "    plt.plot([x1,x2,x2,x1,x1],[y1,y1,y2,y2,y1],\"r\")\n",
        "\n",
        "idx = 1\n",
        "# Array de l'image\n",
        "im = load_image(df.filename[idx])\n",
        "# Coordonnées de la bounding box\n",
        "bbox = df[['xmoy', 'ymoy', 'w', 'h']].values[idx]\n",
        "# Afficher l'image ainsi que la bounding box\n",
        "show_bounding_box(im, bbox)"
      ],
      "metadata": {
        "id": "pUlSotwtY3Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "À l'aide des fonctions définies précédemment, charger une image et afficher sa bounding box."
      ],
      "metadata": {
        "id": "usN8yNRQY53h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Array de l'image\n",
        "im = load_image(df.filename[8])\n",
        "# Coordonnées de la bounding box\n",
        "bbox = df[['xmoy', 'ymoy', 'w', 'h']].values[8]\n",
        "# Afficher l'image ainsi que la bounding box\n",
        "show_bounding_box(im, bbox, normalised=True)\n"
      ],
      "metadata": {
        "id": "In9Y-thMY6-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exécuter la cellule suivante pour afficher aléatoirement des images avec leur bounding box."
      ],
      "metadata": {
        "id": "4UmnPf_sY9aQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12,5))\n",
        "for j, i in enumerate(np.random.randint(0, len(df), size=[8])):\n",
        "    plt.subplot(2,4,j+1)\n",
        "    plt.axis('off')\n",
        "    im = load_image(df.filename[i])\n",
        "    bbox = df[['xmoy', 'ymoy', 'w', 'h']].values[i]\n",
        "    show_bounding_box(im, bbox, normalised=True)"
      ],
      "metadata": {
        "id": "y2IHlCw4Y99u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution des variables cibles.\n",
        "\n",
        "Une étape importante dans un problème de régression est d'étudier la distribution de nos variables cibles pour vérifier si certaines valeurs ne sont pas sous-représentées.\n",
        "\n",
        "Afficher la distribution de xmoy et ymoy.\n",
        "Où se trouve généralement la bounding box dans l'image ?"
      ],
      "metadata": {
        "id": "ihokxkQTZBby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.displot(df.xmoy)\n",
        "plt.show()\n",
        "sns.displot(df.ymoy)\n",
        "plt.show()\n",
        "## La bounding box se trouve généralement au centre de l'image."
      ],
      "metadata": {
        "id": "YgKo8ZJUZDpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afficher la distribution de w et h.\n",
        "Est-ce que les petits objets sont bien représentés ?"
      ],
      "metadata": {
        "id": "827znYPQZGZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(df.w)\n",
        "plt.show()\n",
        "sns.displot(df.h)\n",
        "plt.show()\n",
        "## Les objets ont généralement la même taille et les petits objets ne sont pas beaucoup réprésentés.\n"
      ],
      "metadata": {
        "id": "exPF9Z93ZERp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Charger le jeu de données\n",
        "Le jeu de données originel est composé de 150 000 images. Pour charger l'ensemble des images en mémoire, il est alors nécessaire d'avoir au moins 40 Go de RAM et prend 2 heures de chargement. Deux solutions sont disponibles :\n",
        "\n",
        "Comme le jeu de données est trop lourd, choisir un sous-échantillon (ex : 30 000 images) et le charger en mémoire.\n",
        "Charger les images pendant l'entraînement à l'aide d'un générateur.\n",
        "Il est possible de paralléliser le chargement/preprocessing des données et l'entraînement du modèle. Dans cette configuration, le modèle chargera le prochain batch en même temps que l'entraînement du modèle sur le batch actuel.\n",
        "\n",
        "Dans le cas des images, il est possible d'appliquer ces étapes sans ralentir l'entraînement du modèle. C'est pourquoi, il est toujours préférable de choisir la seconde option qui ne limitera pas la taille du jeu de données.\n",
        "\n",
        "Comme le jeu de test est généralement plus petit et testé à chaque epoch, il est par contre préférable de le charger en mémoire.\n",
        "\n",
        "Séparer le jeu de données df.filepath et les variables cibles en un ensemble d'entraînement X_train_path, y_train, et en un ensemble de test X_test_path, y_test. Nous choisirons un rapport de 80% pour les données d'entraînements et une graine aléatoire 1234.\n",
        "Charger les images de X_test_path redimentionnées à [256,256,3] en mémoire dans la variable X_test."
      ],
      "metadata": {
        "id": "sqCifD_uZQrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "X_train_path, X_test_path, y_train, y_test = train_test_split(df.filename, df[['xmoy', 'ymoy', 'w', 'h']], train_size=0.8, random_state=1234)\n",
        "\n",
        "X_test = []\n",
        "for p in tqdm(X_test_path):\n",
        "    im = load_image(p, (256,256)).numpy().astype(np.uint8)\n",
        "    X_test.append(im)\n",
        "    \n",
        "X_test = np.array(X_test)\n"
      ],
      "metadata": {
        "id": "0PF9PcQIZYak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant les données de test chargées, il est nécessaire de définir un générateur permettant de charger les images à chaque itération du modèle. Pour optimiser le temps de chargement des images en mémoire, il est possible de paralléliser le chargement de chaque image en mémoire en utilisant une structure de multi-thread.\n",
        "\n",
        "Les objets de type dataset sur tensorflow sont capables de le faire à l'aide de l'argument num_parallel_calls de la méthode map.\n",
        "\n",
        "Pour rappel, le constructeur from_tensor_slices du package tensorflow.data.Dataset permet de convertir une liste d'array en un dataset.\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_path, y))\n",
        "La méthode map du dataset permet d'appliquer une fonction à chaque observation de celui-ci. Exemple :\n",
        "\n",
        "dataset = dataset.map(lambda x, y : [load_image(x), y])\n",
        "La méthode batch du dataset permet de regrouper les observations sous forme de batch.\n",
        "\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "Définir un dataset dataset_train de (X_train_path, y_train) à l'aide de la fonction from_tensor_slices.\n",
        "\n",
        "À l'aide de la méthode map, appliquer la fonction load_image à chaque valeur de X_train_path. Pour que le chargement s'éffectue en multi-thread, préciser l'argument num_parallel_calls égale à -1.\n",
        "\n",
        "Regrouper les observations sous forme de batch de taille 32."
      ],
      "metadata": {
        "id": "rpahBwJ1Ze8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "@tf.function\n",
        "def load_image(filepath, resize=(256,256)):\n",
        "    im = tf.io.read_file(folder_images+filepath)\n",
        "    im = tf.image.decode_png(im, channels=3)\n",
        "    return tf.image.resize(im, resize)\n",
        "\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((X_train_path, y_train))\n",
        "\n",
        "dataset_train = dataset_train.map(lambda x, y : [load_image(x), y], num_parallel_calls=-1).batch(32)"
      ],
      "metadata": {
        "id": "8OuJllR6ZbUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exécuter la cellule suivante pour comparer le temps de chargement entre une méthode single-threading et multi-threading."
      ],
      "metadata": {
        "id": "gkqVu_DNZqCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import cv2\n",
        "t0 = time.time()\n",
        "for p in X_test_path.values[:32]:\n",
        "    load_image(p)\n",
        "\n",
        "print('Time to load 32 image in a single thread method :', time.time()-t0, 's')\n",
        "\n",
        "iterator= iter(dataset_train.take(1))\n",
        "t0 = time.time()\n",
        "next(iterator)\n",
        "print('Time for a multi-threading method :', time.time()-t0, 's')\n",
        "\n",
        "#Le gain observé entre une méthode single threading et multi-treading est de l'ordre de x10. \n",
        "#Il est donc beaucoup plus judicieux de charger plusieurs images simultanément qu'une seule à la fois."
      ],
      "metadata": {
        "id": "6dRdNQGHZsNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définition du modèle de détection d'objet \n",
        "\n",
        "Pour rappel, le modèle doit prédire à l'aide de l'image les coordonnées de la bounding box : xmoy, ymoy, w et h.\n",
        "\n",
        "[xmoy,ymoy,w,h]=model(image)\n",
        " \n",
        "Les modèles de classification d'image ou de détection d'objet utilisent généralement une approche par transfer Learning.\n",
        "\n",
        "Quelques rappels sur le transfer leaning\n",
        "L'apprentissage par transfert est le phénomène par lequel un apprentissage nouveau est facilité grâce aux apprentissages antérieurs partageant des similitudes. Par exemple, les connaissances acquises lors de l’apprentissage de la reconnaissance des voitures peuvent s’appliquer lorsqu’on essaie de reconnaître des camions.\n",
        "\n",
        "Les modèles existants (VGG, ResNet, ...) sont composés de deux grandes parties. La première appelée backbone est un ensemble de convolution permettant l'extraction des features de l'image. La seconde est une succession de dense layer qui a pour but de classifier.\n",
        "\n",
        "Les données du nouveau problème doivent être assez semblables avec le jeu de données utilisé pour le pré-entrainement. Dans ce cas, la méthode de transfer learning consiste à utiliser le backbone d'un modèle pré-entraîné comme extraction de features. Ensuite, des couches Dense sont ajoutées pour traiter le problème de classification ou de régression."
      ],
      "metadata": {
        "id": "FPjAijwrZ0ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lors du début de l'apprentissage, il est nécessaire de \"freezer\" (bloquer) les poids de la partie pré-entrainée (backbone) puisqu'ils sont proches des poids optimaux. Puis, au courant de l'entraînement, on peut \"unfreeze\" les couches pour affiner les poids du modèle : cette opération est appelée le fine-tuning.\n",
        "\n",
        "\n",
        "\n",
        "Dans cet exercice, le modèle pré-entraîné sera le EfficientNet puisqu'il a montré de très bon résultat et de très bonnes propriétés pour le transfer learning.\n",
        "\n",
        "Voici un exemple pour charger et freeze les poids d'une modèle pré-entraîner :\n",
        "\n",
        "vgg16 = VGG16(include_top=False, input_shape=(256,256,3))\n",
        "for layer in vgg16.layers:\n",
        "    layer.trainable = False\n",
        "model = Sequential()\n",
        "model.add(vgg16)\n",
        "\n",
        "Charger le modèle EfficientNetB0 de tensorflow.keras.applications.\n",
        "Freezer les poids du modèle.\n",
        "Afficher le résumé du modèle."
      ],
      "metadata": {
        "id": "NhER_XataAMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "# Load the model efficientNet\n",
        "efficientNet = EfficientNetB0(include_top=False, input_shape=(256,256,3))\n",
        "\n",
        "# Freeze the blackbone\n",
        "for layer in efficientNet.layers:\n",
        "    layer.trainable = False\n"
      ],
      "metadata": {
        "id": "6Be8UpGBaMWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partie régression\n",
        "Ajouter le modèle à un objet Sequential qui portera le nom de model.\n",
        "Ajouter à ce modèle une couche GlobalAveragePooling2D.\n",
        "Puis, ajouter quelques couches Dense et Dropout.\n",
        "Finir par une couche Dense avec 4 neurones et une fonction activation 'linear'."
      ],
      "metadata": {
        "id": "6KCzTtM4aip5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Conv2D, MaxPooling2D, BatchNormalization, LeakyReLU, Flatten\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(efficientNet)\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(4, activation='linear'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "oOef7DMUajsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définition de la fonction de perte\n",
        "Quantifier l'erreur d'un modèle n'est pas toujours chose aisée. En effet, sur notre problématique, faut-il quantifier différemment l'erreur sur les coordonnées que sur la largeur/hauteur de l'objet ? Faut-il dans un premier temps enlever l'erreur sur la largeur/hauteur, pour laisser le modèle se concentrer sur les coordonnées ? Comme nous sommes face à une problématique de régression simple, l'erreur quadratique moyenne peut-elle simplement convenir ?\n",
        "\n",
        "Il n'y a pas de réponse simple à ce type de question, c'est très souvent l'expérience et les itérations qui y répondront.\n",
        "\n",
        "Définir une fonction de perte loss_function de votre choix avec comme argument y_true et y_pred. Attention à bien n'utiliser que des fonctions venant du package tensorflow.\n",
        "Compiler le modèle avec votre fonction de perte loss_function et avec un optimizer 'adam'."
      ],
      "metadata": {
        "id": "sifX61uXapKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "\n",
        "lambda_regression=10\n",
        "\n",
        "def loss_function(y_true, y_pred):\n",
        "    return lambda_regression*tf.reduce_mean(tf.square(y_true-y_pred), axis=-1)\n",
        "\n",
        "# def loss_function(y_true, y_pred):\n",
        "#     return lambda_coord*tf.reduce_mean(tf.square(y_true[...,:2]-y_pred[...,:2]), axis=-1) + lambda_largeur*tf.reduce_mean(tf.square(y_true[...,2:4]-y_pred[...,2:4]), axis=-1)\n",
        "\n",
        "\n",
        "model.compile(loss=loss_function, optimizer=Adam(1e-3))"
      ],
      "metadata": {
        "id": "xFQz5GM6ap3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entraînement du modèle\n",
        "Entraîner le modèle à l'aide la méthode fit_generator sur 20 epochs."
      ],
      "metadata": {
        "id": "RDkCAuRwatQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_generator(dataset_train, epochs=20)"
      ],
      "metadata": {
        "id": "xWVnZkTiayJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tester le modèle sur images de X_test en affichant la bounding box associée."
      ],
      "metadata": {
        "id": "P24xUW_Xa0lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def show_img(img, model):\n",
        "    plt.imshow(img)\n",
        "    t0=time.time()\n",
        "    x, y, w, h = model.predict(np.array([img], dtype=np.float32))[0]\n",
        "    print(x, y, w, h)\n",
        "    print(\"Execution time :\",time.time()-t0,\"secondes\")\n",
        "    show_bounding_box(img/255, [x,y,w,h])\n",
        "    plt.show()\n",
        "    \n",
        "## Exemple :\n",
        "show_img(X_test[3], model)"
      ],
      "metadata": {
        "id": "ArwDmxAGa1KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exécuter la cellule suivante pour activer la webcam sur le notebook jupyter. Le code javascript va enregistrer l'image venant de votre webcam à chaque fois que le bouton Snap Photo sera pressé. L'image sera stockée dans la variable imageWebCam."
      ],
      "metadata": {
        "id": "hmQcUO5Da_fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from IPython.display import HTML\n",
        "from PIL import Image\n",
        "import base64, io\n",
        "import numpy as np\n",
        "\n",
        "main_text = \"\"\"\n",
        "<style type=\"text/css\">\n",
        "    canvas {\n",
        "        display: none;\n",
        "    }\n",
        "    </style>\n",
        "\n",
        "<video id=\"video\" width=\"320\" height=\"240\" autoplay></video>\n",
        "<button id=\"snap\">Snap Photo</button>\n",
        "<canvas id=\"canvas\" width=\"320\" height=\"240\"></canvas>\n",
        "\n",
        "<script>\n",
        "// Grab elements, create settings, etc.\n",
        "var video = document.getElementById('video');\n",
        "\n",
        "// Get access to the camera!\n",
        "if(navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {\n",
        "    // Not adding `{ audio: true }` since we only want video now\n",
        "    navigator.mediaDevices.getUserMedia({ video: true }).then(function(stream) {\n",
        "        //video.src = window.URL.createObjectURL(stream);\n",
        "        //video.play();\n",
        "        video.srcObject=stream;\n",
        "        video.play();\n",
        "    });\n",
        "}\n",
        "\n",
        "// Elements for taking the snapshot\n",
        "var canvas = document.getElementById('canvas');\n",
        "var context = canvas.getContext('2d');\n",
        "var video = document.getElementById('video');\n",
        "var image = canvas.toDataURL(\"image/png\");\n",
        "IPython.notebook.kernel.execute(\"image = '\" + image + \"'\")\n",
        "\n",
        "// Trigger photo take\n",
        "document.getElementById(\"snap\").addEventListener(\"click\", function() {\n",
        "\tcontext.drawImage(video, 0, 0, 320, 240);\n",
        "    var myCanvas = document.getElementById('canvas');\n",
        "    var image = myCanvas.toDataURL(\"image/png\");\n",
        "    IPython.notebook.kernel.execute(\"print('testing')\")\n",
        "    IPython.notebook.kernel.execute(\"imageWebCam = '\" + image + \"'\")\n",
        "});\n",
        "</script>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def show_bboxes_webcam(model):\n",
        "    img = np.array(Image.open(io.BytesIO(base64.b64decode(imageWebCam.split(',')[1]))))[:,:,0:3]\n",
        "    img = tf.image.resize(img, (256,256))\n",
        "    show_img(img, model)\n",
        "    \n",
        "HTML(main_text)"
      ],
      "metadata": {
        "id": "Yh3Kgxaua-wW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}