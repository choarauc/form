{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reco_vocale_WaneNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMpoNHyPZaTaOLV97aQou89",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choarauc/form/blob/main/reco_vocale_WaneNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://datascientest.fr/train/assets/python_nlp_speech_text_correspondance.png"
      ],
      "metadata": {
        "id": "jFYF-l3lKIdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modèle pour la reconnaissance manuscrite\n",
        "\n",
        "séparer l'image en fragment découpé de gauche à droite et d'extraire des features en utilisant un réseau convolutionnel. Puisque, le texte manuscrit est écrit de gauche à droite, le résultat peut être alors représenté sous forme d'une séquence temporelle.\n",
        "\n"
      ],
      "metadata": {
        "id": "A5aIS2jhKQxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "afficher une aperçu du modèle de reconnaissance optique."
      ],
      "metadata": {
        "id": "1zicFvKeKY-s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXEaBAyoKA6d",
        "outputId": "6c04a9ee-05b6-46eb-896f-0359cdf192d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 128, 32, 32)       832       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 128, 32, 32)      128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 128, 32, 32)       0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 64, 16, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 64, 16, 64)        51264     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 64, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 64, 16, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 32, 8, 64)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 32, 8, 128)        73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 32, 8, 128)       512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 32, 8, 128)        0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 32, 4, 128)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 32, 4, 128)        147584    \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 32, 4, 128)       512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 32, 4, 128)        0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 32, 2, 128)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 32, 2, 256)        295168    \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 32, 2, 256)       1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 32, 2, 256)        0         \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 32, 1, 256)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 32, 256)           0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 32, 512)          789504    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32, 54)            27702     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,388,342\n",
            "Trainable params: 1,387,126\n",
            "Non-trainable params: 1,216\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# P(y|X) avec y le caractère et X le fragment de l'image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, MaxPooling2D, LeakyReLU, Lambda, Dense, Dropout\n",
        "from tensorflow.keras.layers import GRU, Bidirectional\n",
        "from keras.utils.vis_utils import plot_model\n",
        "numHidden = 256\n",
        "alphabet = ' abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "\n",
        "model_ocr = tf.keras.Sequential()\n",
        "# Convolution Part : Extraction Feature\n",
        "# Layer 1\n",
        "model_ocr.add(Conv2D(filters=32, kernel_size=(5,5), padding='SAME', input_shape = (128, 32, 1)))\n",
        "\n",
        "model_ocr.add(BatchNormalization())\n",
        "model_ocr.add(LeakyReLU())\n",
        "model_ocr.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "# Layer 2\n",
        "model_ocr.add(Conv2D(filters=64, kernel_size=(5,5), padding='SAME'))\n",
        "model_ocr.add(BatchNormalization())\n",
        "model_ocr.add(LeakyReLU())\n",
        "model_ocr.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "# Layer 3\n",
        "model_ocr.add(Conv2D(filters=128, kernel_size=(3,3), padding='SAME'))\n",
        "model_ocr.add(BatchNormalization())\n",
        "model_ocr.add(LeakyReLU())\n",
        "model_ocr.add(MaxPooling2D(pool_size=(1,2), strides=(1,2)))\n",
        "\n",
        "# Layer 4\n",
        "model_ocr.add(Conv2D(filters=128, kernel_size=(3,3), padding='SAME'))\n",
        "model_ocr.add(BatchNormalization())\n",
        "model_ocr.add(LeakyReLU())\n",
        "model_ocr.add(MaxPooling2D(pool_size=(1,2), strides=(1,2)))\n",
        "\n",
        "# Layer 5\n",
        "model_ocr.add(Conv2D(filters=256, kernel_size=(3,3), padding='SAME'))\n",
        "model_ocr.add(BatchNormalization())\n",
        "model_ocr.add(LeakyReLU())\n",
        "model_ocr.add(MaxPooling2D(pool_size=(1,2), strides=(1,2)))\n",
        "\n",
        "\n",
        "# Remove axis 2\n",
        "model_ocr.add(Lambda(lambda x :tf.squeeze(x, axis=2)))\n",
        "\n",
        "# Bidirectionnal RNN\n",
        "model_ocr.add(Bidirectional(GRU(numHidden, return_sequences=True)))\n",
        "# Classification of characters\n",
        "model_ocr.add(Dense(len(alphabet)+1))\n",
        "\n",
        "model_ocr.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://datascientest.fr/train/assets/exam_tensorflow_all_model.png"
      ],
      "metadata": {
        "id": "9TG5ddkOKvU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modèle de reconnaissance vocal : WaveNet \n",
        "\n"
      ],
      "metadata": {
        "id": "pIz8vWQOKfO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "X = np.expand_dims(np.arange(11, dtype=float), -1)/1\n",
        "\n",
        "layer_conv1d = tf.keras.layers.Conv1D(1, 2, padding='same', use_bias=False, dilation_rate=2)\n",
        "layer_conv1d.build([1, 11, 1])\n",
        "layer_conv1d.weights[0].assign(np.ones([2,1,1]))\n",
        "with tf.device('/cpu:0'):\n",
        "    y = layer_conv1d(np.array([X])).numpy()\n",
        "\n",
        "print('Input :')\n",
        "print(X, '\\n')\n",
        "print('Output :')\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vz-xgIfK3T3",
        "outputId": "2c5256f8-b01d-41fd-e16f-746264db7645"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input :\n",
            "[[ 0.]\n",
            " [ 1.]\n",
            " [ 2.]\n",
            " [ 3.]\n",
            " [ 4.]\n",
            " [ 5.]\n",
            " [ 6.]\n",
            " [ 7.]\n",
            " [ 8.]\n",
            " [ 9.]\n",
            " [10.]] \n",
            "\n",
            "Output :\n",
            "[[[ 1.]\n",
            "  [ 2.]\n",
            "  [ 4.]\n",
            "  [ 6.]\n",
            "  [ 8.]\n",
            "  [10.]\n",
            "  [12.]\n",
            "  [14.]\n",
            "  [16.]\n",
            "  [18.]\n",
            "  [ 9.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "définir les classes de notre modèle de reconnaissance vocal."
      ],
      "metadata": {
        "id": "APKNfF9hLEvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AtrousConv1D(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 filters,\n",
        "                 kernel_size,\n",
        "                 dilation_rate,\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "                 causal=True\n",
        "                ):\n",
        "        super(AtrousConv1D, self).__init__()\n",
        "        \n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation_rate = dilation_rate\n",
        "        self.causal = causal\n",
        "        \n",
        "        # Convolution with dilation\n",
        "        self.conv1d = tf.keras.layers.Conv1D(\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            dilation_rate=dilation_rate,\n",
        "            padding='valid' if causal else 'same',\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer\n",
        "        )\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        # If padding 'valid', the shape of tensor change.\n",
        "        if self.causal:\n",
        "            padding = (self.kernel_size - 1) * self.dilation_rate\n",
        "            inputs = tf.pad(inputs, tf.constant([(0, 0,), (1, 0), (0, 0)]) * padding)\n",
        "        \n",
        "        return self.conv1d(inputs)\n",
        "    \n",
        "    \n",
        "class ResidualBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, dilation_rate, causal, **kwargs):\n",
        "        super(ResidualBlock, self).__init__(**kwargs)\n",
        "        \n",
        "        self.batch_normalization = tf.keras.layers.BatchNormalization()\n",
        "        \n",
        "        # First convolution of ResidualBloack\n",
        "        self.dilated_conv1 = AtrousConv1D(\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            dilation_rate=dilation_rate,\n",
        "            causal=causal\n",
        "        )\n",
        "        \n",
        "        # Second convolution of ResidualBloack\n",
        "        self.dilated_conv2 = AtrousConv1D(\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            dilation_rate=dilation_rate,\n",
        "            causal=causal\n",
        "        )\n",
        "        \n",
        "        self.out = tf.keras.layers.Conv1D(\n",
        "            filters=filters,\n",
        "            kernel_size=1\n",
        "        )\n",
        "        \n",
        "    def call(self, inputs, training=True):\n",
        "        # Normalization of data\n",
        "        data = self.batch_normalization(\n",
        "            inputs\n",
        "        )\n",
        "        # Dilated convolution filters\n",
        "        filters = self.dilated_conv1(data)\n",
        "        filters = tf.nn.tanh(filters)\n",
        "        \n",
        "        # Dilated convolution gates\n",
        "        gates = self.dilated_conv2(data) \n",
        "        gates = tf.nn.sigmoid(gates)\n",
        "        \n",
        "        # Elem-wise multiply\n",
        "        out = tf.nn.tanh(\n",
        "            self.out(\n",
        "                filters * gates\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        return out + inputs, out\n",
        "    \n",
        "        \n",
        "class ResidualStack(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, dilation_rates, causal, **kwargs):\n",
        "        super(ResidualStack, self).__init__(**kwargs)\n",
        "        \n",
        "        # Definition of all Residual Block\n",
        "        self.blocks = [\n",
        "            ResidualBlock(\n",
        "                filters=filters,\n",
        "                kernel_size=kernel_size,\n",
        "                dilation_rate=dilation_rate,\n",
        "                causal=causal\n",
        "            )\n",
        "            for dilation_rate in dilation_rates\n",
        "        ]\n",
        "        \n",
        "    def call(self, inputs, training=True):\n",
        "        data = inputs\n",
        "        skip = 0\n",
        "        \n",
        "        for block in self.blocks:\n",
        "            # Output of Residual Block\n",
        "            data, current_skip = block(data, training=training)\n",
        "            # add all each skip connection\n",
        "            skip += current_skip\n",
        "\n",
        "        return skip\n",
        "\n",
        "\n",
        "class SpeechNet(tf.keras.Model):\n",
        "    def __init__(self, params, **kwargs):\n",
        "        super(SpeechNet, self).__init__(**kwargs)\n",
        "        \n",
        "        self.batchnormalization1 =tf.keras.layers.BatchNormalization()\n",
        "        \n",
        "        # Expand convolution: extract features\n",
        "        self.expand = tf.keras.layers.Conv1D(\n",
        "            filters = params['stack_filters'],\n",
        "            kernel_size=1,\n",
        "            padding='same'\n",
        "        )\n",
        "        # Definition of all Residual Stack\n",
        "        self.stacks = [\n",
        "            ResidualStack(\n",
        "                filters=params['stack_filters'],\n",
        "                kernel_size=params['stack_kernel_size'],\n",
        "                dilation_rates=params['stack_dilation_rates'],\n",
        "                causal=params['causal_convolutions']\n",
        "            )\n",
        "            for _ in range(params['stacks'])\n",
        "        ]\n",
        "        # Definition of the last convolution\n",
        "        self.out = tf.keras.layers.Conv1D(\n",
        "            filters=len(params['alphabet']) + 1,\n",
        "            kernel_size=1,\n",
        "            padding='same'\n",
        "        )\n",
        "        \n",
        "        self.batchnormalization2 = tf.keras.layers.BatchNormalization()\n",
        "        \n",
        "    def call(self, inputs, training=True):\n",
        "        # Data Normalization\n",
        "        data = self.batchnormalization1(\n",
        "            inputs\n",
        "        )\n",
        "        \n",
        "        # Right shape for convolution.\n",
        "        if len(data.shape) == 2:\n",
        "            data = tf.expand_dims(data, 0)\n",
        "            \n",
        "        # Extract features    \n",
        "        data = self.expand(data)\n",
        "        \n",
        "        # Residual Stack\n",
        "        for stack in self.stacks:\n",
        "            data = stack(data, training=training)\n",
        "        \n",
        "        # Data Normalization\n",
        "        data = self.batchnormalization2(\n",
        "            data\n",
        "        )\n",
        "        \n",
        "        return self.out(data) + 1e-8"
      ],
      "metadata": {
        "id": "7wIuIK6lLGUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "modele de reconnaissance vocale"
      ],
      "metadata": {
        "id": "lNN9Vj0QLJ8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'max_wave_length': 20,\n",
        "    'alphabet': ' !\"&\\',-.01234:;\\\\abcdefghijklmnopqrstuvwxyz',\n",
        "    'causal_convolutions': False,\n",
        "    'stack_dilation_rates': [1, 3, 9, 27],\n",
        "    'stacks': 6,\n",
        "    'stack_kernel_size': 7,\n",
        "    'stack_filters': 3*128,\n",
        "    'sampling_rate': 16000,\n",
        "    'n_fft': 160*8,\n",
        "    'frame_step': 160*4,\n",
        "    'lower_edge_hertz': 0,\n",
        "    'upper_edge_hertz': 8000,\n",
        "    'num_mel_bins': 160\n",
        "}\n",
        "\n",
        "model = SpeechNet(params)\n",
        "number_exploited_data = params['max_wave_length']*params['sampling_rate']-params['n_fft']\n",
        "lengths = int(number_exploited_data/params['frame_step']+1)\n",
        "model(np.random.uniform(size=[1, lengths, params['num_mel_bins']]))\n",
        "model.load_weights('model/model.h5')"
      ],
      "metadata": {
        "id": "CXCUuBQoLMRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load_wave charger les dix premiers fichiers audio dans la variable X.\n",
        "\n",
        "def load_wave(path_audio, params):\n",
        "    # Load audio\n",
        "    wave, fe = load_audio(path_audio)\n",
        "    # Return None for a too long file\n",
        "    if len(wave) > params['max_wave_length']*fe:\n",
        "        print('Shape invalid')\n",
        "        return None\n",
        "   \n",
        "    # After this transformation add zeroes to have the right shape\n",
        "    else :\n",
        "        return np.concatenate([wave, np.zeros(params['max_wave_length']*fe - len(wave))])\n",
        "    \n",
        "X_audio = [load_wave(p, params) for p in df.audio_path[:10]]\n",
        "y = df.text[:10]"
      ],
      "metadata": {
        "id": "zAPDbXQtLPKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inférence - CTC Decoder"
      ],
      "metadata": {
        "id": "laTH_xXRLd-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "les fichiers audios X_audio sous la forme d'un tableau array de log mel spectrogramme dans la variable X."
      ],
      "metadata": {
        "id": "khavH_CZLVj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fe = 16000\n",
        "X = np.array([logMelSpectrogram(audio, params, fe) for audio in X_audio])"
      ],
      "metadata": {
        "id": "44gHKNntLWVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "la méthode de Best path decoding sur la matrice p_matrix."
      ],
      "metadata": {
        "id": "LO9h_0zvLa9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_matrix = np.array(\n",
        "        [[0.3, 0.1, 0.05, 0.05, 0.5],\n",
        "        [0.5, 0.05, 0.05, 0.1, 0.3],\n",
        "        [0.3, 0.1, 0.05, 0.05, 0.5],\n",
        "        [0.5, 0.05, 0.05, 0.1, 0.3],\n",
        "        [0.3, 0.1, 0.05, 0.05, 0.5],\n",
        "        [0.5, 0.05, 0.05, 0.1, 0.3],\n",
        "        [0.3, 0.1, 0.05, 0.05, 0.5],\n",
        "        [0.5, 0.05, 0.05, 0.1, 0.3],\n",
        "        [0.3, 0.1, 0.05, 0.05, 0.5],\n",
        "        [0.5, 0.05, 0.05, 0.1, 0.3]])\n",
        "\n",
        "print('Transpose probabilty matrix')\n",
        "print(p_matrix.T)\n",
        "# Example of greedy_decoder\n",
        "def greedy_decoder(data):\n",
        "    # index for largest probability each row\n",
        "    return [np.argmax(s) for s in data]\n",
        "\n",
        "greedy_decoder(p_matrix)"
      ],
      "metadata": {
        "id": "oS9Gs1-mLhE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beam search decoder"
      ],
      "metadata": {
        "id": "n6gUGygjLjii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of beam search decoder\n",
        "def beam_search_decoder(data, k):\n",
        "    sequences = [[list(), 1.0]]\n",
        "    # walk over each step in sequence\n",
        "    for row in data:\n",
        "        all_candidates = list()\n",
        "        # expand each current candidate\n",
        "        for i in range(len(sequences)):\n",
        "            seq, score = sequences[i]\n",
        "            for j in range(len(row)):\n",
        "                candidate = [seq + [j], score * row[j]]\n",
        "                all_candidates.append(candidate)\n",
        "        # order all candidates by score\n",
        "        ordered = sorted(all_candidates, key=lambda tup:tup[1], reverse=True)\n",
        "        # select k best\n",
        "        sequences = ordered[:k]\n",
        "    return sequences\n",
        " \n",
        "\n",
        "print(p_matrix.T)\n",
        "\n",
        "\n",
        "# decode sequence\n",
        "result = beam_search_decoder(p_matrix, 3)\n",
        "\n",
        "# print result\n",
        "for seq in result:\n",
        "    print(seq)"
      ],
      "metadata": {
        "id": "c3kT8hHNLlhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a fonction ctc_beam_search_decoder de tf.nn retourne un tuple (decoded, log_probabilities) à l'aide d'un décodeur beam search decoder. L'élément decoded est un SparseTensor contenant la sortie décodée. Et, log_probabilities est le log de la probabilité présenté dans la partie précédente.\n",
        "\n",
        "La fonction a comme argument :\n",
        "\n",
        "inputs: Tensor de forme [max_time x batch_size x num_classes] représentant la sortie de notre réseau de neuronne.\n",
        "sequence_length: Vecteur de taille [batch_size] représentant la longueur de la séquence pour chaque élément dans le batch de données.\n",
        "beam_width: Nombre de meilleurs chemins uniques."
      ],
      "metadata": {
        "id": "xtk9SBO_LpgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " définir le beam search decoder de tensorflow."
      ],
      "metadata": {
        "id": "OH8DnUXBLqcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_codes(codes, charList):\n",
        "    table = tf.lookup.StaticHashTable(\n",
        "        tf.lookup.KeyValueTensorInitializer(\n",
        "            np.arange(len(charList)),\n",
        "            charList,\n",
        "            key_dtype=tf.int32\n",
        "        ),\n",
        "        '',\n",
        "        name='id2char'\n",
        "    )\n",
        "    return table.lookup(codes)\n",
        "\n",
        "def greedy_decoder(logits, params):\n",
        "    # ctc beam search decoder\n",
        "    predicted_codes, _ = tf.nn.ctc_beam_search_decoder(\n",
        "        inputs = tf.transpose(logits, (1, 0, 2)),\n",
        "        sequence_length = [logits.shape[1]]*logits.shape[0],\n",
        "        beam_width = 100,\n",
        "        top_paths = 1\n",
        "    )\n",
        "    # convert to int32\n",
        "    codes = tf.cast(predicted_codes[0], tf.int32)\n",
        "    \n",
        "    # Decode the index of caracter\n",
        "    text = decode_codes(codes, list(params['alphabet']))\n",
        "    \n",
        "    # Convert a SparseTensor to string\n",
        "    text = tf.sparse.to_dense(text).numpy().astype(str)\n",
        "    \n",
        "    return list(map(lambda x: ''.join(x), text))"
      ],
      "metadata": {
        "id": "Om2lFStdLqF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Résultat"
      ],
      "metadata": {
        "id": "HcrUbdJaLupd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_logit = model(X)\n",
        "transcriptions = greedy_decoder(y_logit, params)\n",
        "transcriptions"
      ],
      "metadata": {
        "id": "jDOZ2xOCLvI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prédiction et la vrai transcription du premier fichier audio de X_audio."
      ],
      "metadata": {
        "id": "iyMOXz8JLw5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_id = 0\n",
        "print('Prediction :\\n', transcriptions[sample_id], '\\n')\n",
        "print('Real Transcription :\\n', y[sample_id])\n",
        "Audio(X_audio[sample_id], rate=fe)"
      ],
      "metadata": {
        "id": "8qHPhi8ELyuv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}